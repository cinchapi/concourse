/*
 * Copyright (c) 2013-2022 Cinchapi Inc.
 *
 * Licensed under the Apache License, Version 2.0 (the "License");
 * you may not use this file except in compliance with the License.
 * You may obtain a copy of the License at
 *
 * http://www.apache.org/licenses/LICENSE-2.0
 *
 * Unless required by applicable law or agreed to in writing, software
 * distributed under the License is distributed on an "AS IS" BASIS,
 * WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.
 * See the License for the specific language governing permissions and
 * limitations under the License.
 */
package com.cinchapi.concourse.server.storage;

import java.util.AbstractSet;
import java.util.ArrayList;
import java.util.Collection;
import java.util.HashMap;
import java.util.Iterator;
import java.util.LinkedList;
import java.util.List;
import java.util.Map;
import java.util.Set;
import java.util.concurrent.atomic.AtomicReference;
import java.util.function.Supplier;

import javax.annotation.concurrent.NotThreadSafe;

import com.cinchapi.concourse.server.GlobalState;
import com.cinchapi.concourse.server.storage.temp.Write;
import com.cinchapi.concourse.util.Logger;
import com.google.common.base.Preconditions;
import com.google.common.collect.LinkedHashMultimap;
import com.google.common.collect.Multimap;
import com.google.common.hash.BloomFilter;
import com.google.common.hash.Funnels;
import com.google.common.hash.HashCode;

/**
 * Analyzes and profiles a collection of {@link WriteStream WriteStreams} and
 * provides various capabilities and insights about the contained {@link Write
 * Writes}.
 * <p>
 * This profiler is not thread safe and assumes that the input collection of
 * {@link WriteStream streams} is externally synchronized.
 * </p>
 *
 * @author Jeff Nelson
 */
@NotThreadSafe
public final class WriteStreamProfiler<T extends WriteStream> {

    /*
     * NOTE: In the future, it may be beneficial for this class to provide
     * deeper analysis about the WriteStream like temporal distribution, etc
     */

    /**
     * The duplicates found during the {@link #dedupe()}, as a mapping from each
     * {@link Write} to the streams where it was found duplicated.
     */
    private Multimap<Write, T> duplicates;

    /**
     * The collection of {@link WriteStream WriteStreams}.
     */
    private final Collection<T> streams;

    /**
     * Tracks all the {@link Write#hash() hashes) that have been seen.
     */
    private final Set<HashCode> hashes;

    /**
     * Construct a new instance.
     * 
     * @param streams
     * @param factory
     */
    public WriteStreamProfiler(Collection<T> streams) {
        this.streams = streams;
        this.hashes = new HashTracker(
                (GlobalState.BUFFER_PAGE_SIZE / Write.MINIMUM_SIZE)
                        * streams.size());
        this.duplicates = null;
    }

    /**
     * Return a {@link Multimap} of all each {@link Write} that was found to be
     * duplicated on the last {@link #dedupe()} mapped to the streams where it
     * was duplicated.
     * 
     * @return the duplicate {@link Write Writes}
     */
    public Multimap<Write, T> duplicates() {
        Preconditions.checkState(duplicates != null,
                "The deduplicate method must be run prior to accessing the duplicates");
        return duplicates;
    }

    /**
     * Search through all of the {@link #streams} for any {@link Write Writes}
     * that are not balanced across the entire collection.
     * <p>
     * Every {@link Write} about a topic, must offset any previous {@link Write
     * writes} about that topic (e.g., a REMOVE is only valid if there was a
     * previous ADD and an ADD is only valid if there was no previous
     * {@ink Write} for that topic or there was a previous REMOVE). When the
     * {@link Write writes} for a topic across the {@link #streams} are properly
     * offset, they are known to be "balanced". The state of the data is only
     * valid if all {@link Write writes} for all topics are balanced.
     * </p>
     * <p>
     * Each {@link WriteStream} that contains at least one {@link Write} that is
     * not properly offset is returned in a {@link Map} from the offending
     * stream to another one, generated by {@code factory} and clear of all
     * imbalances.
     * </p>
     * <p>
     * The collection of {@link #streams} is <strong>NOT</strong> modified and
     * the "cleaned" streams that are {@link Map#values() values} in the
     * returned {@link Map} are only guaranteed to have had balanced
     * {@link Write writes} from he original stream
     * {@link WriteStream#append(Write) appended} (e.g. no
     * sync or save). If desired, the caller must manually update external
     * collections with cleaned streams.
     * </p>
     * 
     * @param factory used to create a new {@link WriteStream} where
     *            non-duplicate data is staged.
     * @return a {@link Map} from each stream with duplicate data to a replicate
     *         stream where duplicates have been removed
     */
    public Map<T, T> balance(Supplier<T> factory) {
        Map<T, T> repaired = new HashMap<>();

        // Track the running state of each Write and whether it "exists" based
        // on the sequence of revisions across the streams.
        Map<Write, Boolean> state = new HashMap<>();
        streams.forEach(stream -> {
            AtomicReference<T> staging = new AtomicReference<>(null);
            List<Write> balanced = new ArrayList<>();
            stream.writes().forEach(write -> {
                boolean exists = state.computeIfAbsent(write, $ -> false);
                if((write.getType() == Action.ADD && !exists)
                        || (write.getType() == Action.REMOVE && exists)) {
                    state.put(write, !exists);
                    if(staging.get() != null) {
                        staging.get().append(write);
                    }
                    else {
                        balanced.add(write);
                    }
                }
                else {
                    Logger.warn("Found unoffset Write {} in {}", write, stream);
                    if(staging.get() == null) {
                        staging.set(factory.get());
                        for (Write seen : balanced) {
                            staging.get().append(seen);
                        }
                        balanced.clear();
                    }
                }
            });
            if(staging.get() != null) {
                repaired.put(stream, staging.get());
            }
            else {
                Logger.info("No duplicate Writes found in {}", stream);
            }
        });
        return repaired;
    }

    /**
     * Search through all of the {@link #streams} for any {@link Write Writes}
     * that are duplicated across the entire collection. Each
     * {@link WriteStream} that contains at least one duplicate {@link Write} is
     * returned in a {@link Map} from the offending stream to another one,
     * generated by {@code factory} that is clear of all duplicates.
     * <p>
     * The collection of {@link #streams} is <strong>NOT</strong> modified and
     * the "cleaned" streams that are {@link Map#values() values} in the
     * returned {@link Map} are only guaranteed to have had non-duplicate
     * {@link Write writes} from the original stream
     * {@link WriteStream#append(Write) appended} (e.g. no sync or save). If
     * desired, the caller must manually update external collections with
     * cleaned streams.
     * </p>
     * 
     * 
     * @param factory used to create a new {@link WriteStream} where
     *            non-duplicate data is staged.
     * @return a {@link Map} from each stream with duplicate data to a replicate
     *         stream where duplicates have been removed
     */
    public Map<T, T> deduplicate(Supplier<T> factory) {
        duplicates = LinkedHashMultimap.create();
        Map<T, T> deduped = new HashMap<>();
        streams.forEach(stream -> {
            AtomicReference<T> staging = new AtomicReference<>(null);
            List<Write> unique = new ArrayList<>();
            stream.writes().forEach(write -> {
                HashCode hash = write.hash();
                if(hashes.add(hash)) {
                    if(staging.get() != null) {
                        staging.get().append(write);
                    }
                    else {
                        unique.add(write);
                    }
                }
                else {
                    Logger.warn("Found duplicate Write {} in {}", write,
                            stream);
                    if(staging.get() == null) {
                        staging.set(factory.get());
                        for (Write seen : unique) {
                            staging.get().append(seen);
                        }
                        unique.clear();
                    }
                    duplicates.put(write, stream);

                }
            });
            if(staging.get() != null) {
                deduped.put(stream, staging.get());
            }
            else {
                Logger.info("No duplicate Writes found in {}", stream);
            }
        });
        return deduped;
    }

    /**
     * A {@link Set} that can efficiently store a range of {@link Write}
     * {@link Write#hash() hashes}.
     *
     * @author Jeff Nelson
     */
    private static class HashTracker extends AbstractSet<HashCode> {

        /**
         * A {@link BloomFilter} to speed up calls to {@link #contains(Object)}.
         */
        private final BloomFilter<byte[]> filter;

        /**
         * Each hash that has been {@link #add(Long) added}.
         */
        private final LinkedList<HashCode> hashs;

        /**
         * Construct a new instance.
         * 
         * @param numBlocks
         */
        public HashTracker(int expectedInsertions) {
            this.hashs = new LinkedList<>();
            this.filter = BloomFilter.create(Funnels.byteArrayFunnel(),
                    expectedInsertions);

        }

        @Override
        public boolean add(HashCode e) {
            if(!contains(e)) {
                filter.put(e.asBytes());
                hashs.add(e);
                return true;
            }
            else {
                return false;
            }
        }

        @Override
        public boolean contains(Object o) {
            if(o instanceof HashCode) {
                HashCode hash = (HashCode) o;
                if(filter.mightContain(hash.asBytes())) {
                    return hashs.contains(hash);
                }
            }
            return false;
        }

        @Override
        public Iterator<HashCode> iterator() {
            return hashs.iterator();
        }

        @Override
        public int size() {
            return hashs.size();
        }
    }

}
